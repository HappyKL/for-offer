# 容器网络

# 《Kubernetes网络权威指南：基础、原理与实践》

## 2.2 Docker的四大网络模式

容器的网络方案分为三大部分：

- 单机的容器间通信
- 跨主机的容器间通信
- 容器与主机间通信

在使用docker run命令创建容器时，可以使用--network选项来指定容器的网络模式。有四种网络模式：

- bridge模式
- host模式
- container模式
- none模式

在安装完Docker之后，Docker Daemon会在宿主机上自动创建三个网络，分别是bridge网络，host网络和none网络，可以使用docker network ls命令查看

![image-20200306021236088](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200306021236088.png)



### 2.2.1 bridge模式

Docker默认的网络模式

docker0网桥+veth pair  

veth pair ：连接两个ns，一端在网桥上，一端在容器里

同主机间容器直接通信；访问非本机容器网段需要经过docker0网关转发

可以看容器里路由信息：

![image-20200306022707501](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200306022707501.png)

### 2.2.2 host模式

与主机共享网络栈，网络配置与host完全一致

### 2.2.3 container模式

使用--network=container:NAME_ID

与指定容器共享一个网络栈network namespace

k8s中pod网络采用的就是这种模式

### 2.2.4 none模式

容器有自己的network namespace，但只有lo回环网络，没有其他网卡，属于完全封闭的网络

## 2.3 常用的Docker网络技巧

### 2.3.1 查看容器IP

```shell
docker inspect -f "{{.NetworkSettings.IPAddress}}" <containerNameOrId>
```

### 2.3.2 端口映射

```shell
# 将主机1234端口映射至容器80端口
docker run -p 1234:80 -d nginx
```

原理是iptables的nat表添加了规则，对访问本机IP地址:1234的数据包进行了DNAT，转换成容器IP:containerport

查看iptables可以看到：output和prerouting链都进行了dnat

![image-20200306030010342](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200306030010342.png)



```shell
# 查看容器端口在主机上的映射
docker port container port-number 
```



### 2.3.3 访问外网

ip_forward + stat/masquerade

通过Linux系统的转发功能实现

```shell
sysctl net.ipv4.ip_forward=1
```

iptables规则（snat/masquerade）：

![image-20200306030402156](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200306030402156.png)



### 2.3.5 自定义网络

```shell
docker network create -d bridge --subnet 172.25.0.0/16 mynet

docker inspect 网络id/网络名 # 查看自定义网络信息

docker network ls # 查看创建的网络

# 删除网络
dcoker network rm 网络id/网络名

# 将容器接入网络
docker network connect mynet containername
# 将容器与网络断开
docker network disconnect mynet containername

```



# 极客时间 《深入剖析Kubernetes》

## 32 浅谈容器网络 （网桥模式）

- 个人理解：docker会在宿主机上创建一个docker0的网桥，容器创建时，会创建VethPair，一端在容器里，一端在宿主机上，分别对应两张虚拟网卡
  - 在容器里可以路由规则，第二条路由规则是对于目的地址是172.17.0.0/16的包，网关是0.0.0.0，这意味着这是一条直连规则，即：凡是匹配到这条规则的IP包，应该经过本机的eth0网卡，经过二层网络直接发往目的主机。二层网络需要目的ip对应的mac地址，因此需要eth0网卡发送一个ARP广播
  - 一旦一张虚拟网卡被插到网桥上，它就变成该网桥的“从设备”。从设备会被剥夺调用网络协议栈处理数据包的资格，从而成为网桥上的一个端口。这个端口只是接收流入的数据包，对于这些数据包的转发或者丢弃，由网桥负责



网络栈：网卡，回环设备，路由表，iptables规则

Veth Pair虚拟设备：相当于连接到不同network nemespace的网线

```shell
# 在宿主机上
$ docker exec -it nginx-1 /bin/bash
# 在容器里
root@2b3c181aecf1:/# ifconfig
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.0.2  netmask 255.255.0.0  broadcast 0.0.0.0
        inet6 fe80::42:acff:fe11:2  prefixlen 64  scopeid 0x20<link>
        ether 02:42:ac:11:00:02  txqueuelen 0  (Ethernet)
        RX packets 364  bytes 8137175 (7.7 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 281  bytes 21161 (20.6 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        
$ route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         172.17.0.1      0.0.0.0         UG    0      0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 eth0
```



```shell
# 在宿主机上
$ ifconfig
...
docker0   Link encap:Ethernet  HWaddr 02:42:d8:e4:df:c1  
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:d8ff:fee4:dfc1/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:309 errors:0 dropped:0 overruns:0 frame:0
          TX packets:372 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0 
          RX bytes:18944 (18.9 KB)  TX bytes:8137789 (8.1 MB)
veth9c02e56 Link encap:Ethernet  HWaddr 52:81:0b:24:3d:da  
          inet6 addr: fe80::5081:bff:fe24:3dda/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:288 errors:0 dropped:0 overruns:0 frame:0
          TX packets:371 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0 
          RX bytes:21608 (21.6 KB)  TX bytes:8137719 (8.1 MB)
          
$ brctl show
bridge name bridge id  STP enabled interfaces
docker0  8000.0242d8e4dfc1 no  veth9c02e56
```



### 同一主机间容器通信

![image-20200306235907037](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200306235907037.png)



可以通过以下命令来查看网络数据包的传输过程，从而验证以上的流程

```shell

# 在宿主机上执行
$ iptables -t raw -A OUTPUT -p icmp -j TRACE
$ iptables -t raw -A PREROUTING -p icmp -j TRACE
```

### 宿主机与本机容器通信

![image-20200307163942702](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307163942702.png)

### 本机容器与另一个宿主机通信

![image-20200307164047751](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307164047751.png)

比如ping 10.168.0.3.  发出的请求数据包，经过Docker0网桥出现在宿主机上，然后根据路由表里的直连路由(10.168.0.0/24 via eth0) ,对10.168.0.3的访问请求就会交给eth0处理

### 跨主机容器间通信

（下一节详细解析）

- 个人理解：通过隧道或路由实现Overlay Network

![image-20200307164620445](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307164620445.png)



## 33 深入解析容器跨主机网络

- 个人理解：这一节讲述了udp模式和VXLAN模式，都是一种“隧道”机制，是很多其他容器网络插件网络的基础，比如Weave的两种模式，以及Docker 的Overlay模式（不太知道这两个例子///）

- 个人理解：udp模式：container1 -> docker0 -> tunnel设备（flannel0，将IP包从内核态传递到用户态） -> flanneld进程(封装原始IP包，从etcd通过目的容器子网所在的网段获取到目的容器所在宿主机的IP) ->  eth0 -> 目的宿主机eth0 -> 目的flanneld进程 -> tunnel设备（flannel0设备，将ip包从用户态发往内核态）-> docker0 -> container2

  共经过三次内核态与用户态的交互，一次是container1的ip包经过docker0进入；一次是flannel0设备将ip包传递给flanneld进程；最后flanneld进程封装好后通过eth0传走

- 个人理解：VXLAN模式：container1 -> docker0 -> vtep设备(flannel.1设备，封装，首先通过flanneld进程维护的路由信息获得vtep设备出口的IP地址，然后通过flanneld进程维护的ARP信息，找到相应的MAC地址并添加，然后添加VXLAN头，然后通过flanneld进程维护的FDB信息添加目的宿主机IP地址，并通过宿主机的ARP学习机制获得目的宿主机的MAC地址，从而进行封装) -> eth0 -> 目的宿主机eth0 (内核拆包后发现VXLAN头，根据VNI的值1发到flannel.1设备)-> vtep设备(flannel.1设备，解封装) -> docker0 -> container2

  共经过一次内核态和用户态的交互，container1的IP包经过docker0进入内核，然后就传走了



Flannel支持三种后端实现：

- VXLAN
- host-gw
- UDP：性能差，已被弃用

### UDP模式

![image-20200307165953406](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307165953406.png)

- flannel0设备是一个Tunnel设备，Tunnel设备是一个工作在三层的虚拟网络设备，TUN设备的功能非常简单，即在操作系统内核和用户应用程序之间传递IP包
- 以flannel0设备为例，发送时，当IP包经过docker0网桥出现在宿主机上后，会根据路由表进入flannel0设备，此时宿主机上的flanneld进程就会收到这个IP包（flannel0设备将操作系统内核的IP包发送给了应用程序flanneld），然后根据在etcd中存储的容器网段找到相对应的宿主机ip地址，然后flanneld进程通过发送UDP包发到目的宿主机上
- 同理，到达目的宿主机后，flanneld进程会接收到数据包，然后解封装后，发到它管理的flannel0设备，（这就是一个从用户态到内核态的流动方向，即从应用程序到内核，TUN设备的功能），之后就会根据路由表到达docke0网桥，之后通过veth pair发送到了对应的容器上。

Flannel UDP模式提供的其实就是一个三层的Overlay网络：即它首先对发出端的IP包进行UDP封装，然后在接收端进行解封装拿到原始的IP包，进而把这个包发送给目标容器。这就好比在两台主机打通了一条隧道，使得两个容器可以直接使用IP地址进行通信，而无需关系容器和宿主机的分布情况。



- 性能差：仅在发出IP包的过程就需要经过三次用户态与内核态的数据拷贝

  - 第一次，用户态的容器进程，发出的IP包经过docker0网桥进入内核态
  - 第二次，IP包经过Tunnel设备回到用户态的flanneld进程
  - 第三次，flanneld进程尽心UDP封包重新进入内核态，将UDP包通过宿主机的eth0发送出去

  ![image-20200307171444123](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307171444123.png)

  （不太明白）此外，Flannel进行UDP封装和解封装的过程，也都是在用户态完成的，在Linux操作系统中，这些上下文切换和用户态操作的代价其实是比较高的

- 优化：进行系统级编程的时候，要减少用户态和内核态的切换次数，并把核心的处理逻辑放在内核态进行

### VXLAN模式

主流的容器网络方案

VXLAN  virtual extensible LAN 虚拟可扩展局域网是Linux内核本身就支持的一种网络虚拟化技术。**VXLAN可以完全在内核态实现上述封装和解封装的工作**，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay network）

- 设计思想：在现有的三层网络之上，“覆盖”一层虚拟的，由内核VXLAN模块负责维护的二层网络，使得连接在这个VXLAN二层网络上的“主机”（虚拟机或者容器均可）之间，可以像在同一个局域网里那样自由通信。
- 为了在二层网络里打通隧道，VXLAN会在宿主机上设置一个特殊的网络设备作为“隧道”的两端，这个设备就叫做VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）
- VTEP设备的作用与UDP模式中flanneld进程非常相似，只不过，它封装的是一个二层数据帧，（flanneld封装的是三层数据帧），而且它工作在内核（VXLAN本身就是Linux内核的一个模块）

![image-20200307174249808](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307174249808.png)

- fannel.1设备就是VXLAN所需的VTEP设备，它既有IP地址，也有MAC地址

- 如何将原始IP包封装后发送到正确的宿主机？VXLAN需要找到隧道的出口，即目的宿主机的VTEP设备

  - 宿主机的VTEP设备是由宿主机上的flanneld进程负责的

  - 当Node2启动并加入Flannel网络之后，在Node1(以及所有其他节点)上，flanneld进程就会添加一条如下的路由规则

    - 规则意思是：凡是发向10.1.16.0/24网段的IP包，都需要经过flannel.1设备发出，发向的网关地址是10.1.16.0。这个网关就是Node2上VTEP设备（flannel.1设备）的IP地址

    ```shell
    $ route -n
    Kernel IP routing table
    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
    ...
    10.1.16.0       10.1.16.0       255.255.255.0   UG    0      0        0 flannel.1
    ```

  - VTEP设备之间，需要想办法组成一个虚拟的二层网络，即通过二层数据帧进行通信。因此源VTEP设备要想办法给原始IP包加上一个目的MAC地址，然后发送给目的VTEP设备，而这个MAC地址应该是目的VTEP设备的MAC地址，那这个地址如何获取呢？

    - MAC地址的获取通常需要ARP表进行获取，这里用到的ARP记录，也是flanneld进程在Node2节点启动时，自动添加到Node1上的，通过ip命令可以看到它：

      ```shell
      # 在Node 1上
      $ ip neigh show dev flannel.1
      10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT
      ```

      可以看到，这并不依赖L3 MISS事件和ARP学习，而会在每台节点启动时把它的VTEP设备对应的ARP记录，直接下放到其他每台宿主机上

  - 有了MAC地址，Linux内核就可以开始二层封包工作了

    ![image-20200307183603680](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307183603680.png)

  - 封装好二层数据包了，但这个数据包并不能在宿主机二层网络里传输，因此还需要进一步封装成宿主机的一个普通的数据帧，通过宿主机eth0网卡进行传输

    - 首先在刚才封装的二层数据包前面加上一个特殊的VXLAN头，头里有一个重要标志VNI，默认值为1，这就是为何宿主机上VTEP设备叫做flannel.1的原因

    - 现在就像UDP模式一样，发起一次普通的UDP连接。但现在的问题是：如何知道目的VTEP设备所在的宿主机的IP地址

      - 在这里，flannel.1设备实际上要扮演一个“网桥”的角色，在二层网络进行UDP包的转发，而在Linux内核里，“网桥”设备进行转发的依据，来自于一个叫做FDB(Forwarding Database)的转发数据库

      - 这个flannel.1“网桥”对应的FDB信息，也是flanneld进程负责维护的，它的内容可以通过bridge fdb命令查看到

        ```shell
        # 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询
        $ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37
        5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent
        ```

        - 发往目的VTEP设备（MAC地址为5e:f8:4f:00:e3:37）的二层数据帧，应该通过flannel.1设备，发往IP地址为10.168.3的主机，即Node2

    - 知道了目的VTEP设备所在的宿主机IP地址后，流程就是一个正常的，宿主机网络的封包工作

  - 最终的数据包大致如下图：

    ![image-20200307184858799](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307184858799.png)

  - 发到Node2后，会到达eth0网卡，这时候Node2的内核网络栈就会发现这个数据帧里有VXLAN Header，并且VNI=1，所以Linux内核会对它进行拆包，拿到里面的内部数据帧，然后根据VNI的值把它交给Node2上的flannel.1设备

  - flannel.1设备会进一步拆包，取出原始IP包，之后通过docker0进入相对应的容器里

综上所述，VXLAN模式组建的覆盖网络，其实就是一个由不同宿主机上的VTEP设备，也就是flannel.1设备组成的虚拟二层网络。对于VTEP设备来说，它发出的内部数据帧，就仿佛是一直在这个虚拟的二层网络里流动，这也正是覆盖网络的含义。

### host-gw模式

见35讲 k8s的三层网络方案

## 34 Kubernetes网络模型与CNI网络插件

- 个人理解：k8s网络模型：与上一节跨主机通信的VXLAN模式一致，只是将docker0网桥转变为cni网桥
- 个人理解：以Flannel为例，说一下Flannel的实现，分为两部分。一部分是实现具体的网络方案，包括flannel.1设备的创建，宿主机路由的配置（根据容器ip知道网关，即目的vtep设备的IP），配置ARP信息（flannel.1设备的ip-->mac），配置FDB表（目的flannel.1设备所对应的宿主机IP）；另一部分是实现网络方案对应的CNI插件，即完成对容器网络栈的配置，即创建cni网桥，并将容器与cni网桥连接起来，流程大致为：kubelet -> CRI(docker下就是dockershim) -> CNI flannel 插件 -> CNI bridge插件 -> dockershim -> kubelet

### k8s的网络模型

上篇文章详细讲解了UDP和VXLAN两种方法，他们有一个共性：

- 用户容器都连接到docker0网桥上，而网络插件则在宿主机上创建一个特殊的设备(UDP模式创建的是TUN设备，VXLAN模式创建的是VTEP设备)，docker0与这个设备之间通过IP转发（路由表）进行协作
- 网络插件真正要做的事，则是通过某种方法，把不同宿主机上的特殊设备连通，从而达到跨主机通信

实际上，这正是Kubernetes对容器网络的主要处理方法，只不过，K8s是通过一个叫做CNI的接口，维护了一个单独的网桥来代替docker0，这个网桥名字就叫做CNI网桥，它在宿主机上的设备名称默认是：cni0

以Flannel的VXLAN模式为例，在k8s里，也是一样的，只不过docker0网桥被替换成了CNI网桥而已

![image-20200307191108029](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200307191108029.png)



### CNI网络插件flannel的实现

之所以要设置一个与docker0网桥功能几乎一样的CNI网桥，主要原因有两个方面：

- 一方面，k8s项目并没有使用Docker的网络模型(CNM)，所以它并不希望，也不具备配置docker0网桥的能力
- 另一方面，还与k8s如何配置Pod，也就是Infra容器的Network Namespace密切相关

CNI的设计思想是：K8s在启动Infra容器之后，就可以直接调用CNI网络插件，为这个Infra容器的Network Namespace，配置符合预期的网络栈



网络栈的配置工作如何完成？

从CNI插件的部署和实现方式谈起：

- 部署k8s时，需要安装kubernetes-cni包，目的就是在宿主机上安装CNI插件所需的基础可执行文件，安装完成后，可以在宿主机/opt/cni/bin目录下看到它们：

  ```shell
  $ ls -al /opt/cni/bin/
  total 73088
  -rwxr-xr-x 1 root root  3890407 Aug 17  2017 bridge
  -rwxr-xr-x 1 root root  9921982 Aug 17  2017 dhcp
  -rwxr-xr-x 1 root root  2814104 Aug 17  2017 flannel
  -rwxr-xr-x 1 root root  2991965 Aug 17  2017 host-local
  -rwxr-xr-x 1 root root  3475802 Aug 17  2017 ipvlan
  -rwxr-xr-x 1 root root  3026388 Aug 17  2017 loopback
  -rwxr-xr-x 1 root root  3520724 Aug 17  2017 macvlan
  -rwxr-xr-x 1 root root  3470464 Aug 17  2017 portmap
  -rwxr-xr-x 1 root root  3877986 Aug 17  2017 ptp
  -rwxr-xr-x 1 root root  2605279 Aug 17  2017 sample
  -rwxr-xr-x 1 root root  2808402 Aug 17  2017 tuning
  -rwxr-xr-x 1 root root  3475750 Aug 17  2017 vlan
  ```

  CNI的基础可执行文件，按功能可以分为三类：

  - 第一类：叫做Main插件，它是用来创建具体网络设备的二进制文件。比如，bridge(网桥设备)、ipvlan、loopback(lo设备)、macvlan、ptp(Veth Pair 设备)以及vlan；
  - 第二类：叫做IPAM（IP Address Management）插件，它是负责分配IP地址的二进制文件。比如，dhcp，这个文件会向DHCP服务器发起请求；host-local，则会使用预先配置的IP地址段来进行分配；
  - 第三类：是由CNI社区维护的内置CNI插件。比如：flannel，就是专门为Flannel项目提供的CNI插件；tuning，是一个通过sysctl调整网络设备的二进制文件；portmap，是一个通过iptables配置端口映射的二进制文件；bandwidth，是一个使用Token Bucket Filter(TBF)来进行限流的二进制文件

- 从这些二进制文件中，可以看到，如果要实现一个给Kubernetes用的容器网络方案，其实需要做两部分工作，以Flannel项目为例：

  - 首先，实现网络方案本身。这一部分需要编写的，其实就是flanneld进程里的主要逻辑。比如，创建和配置flannel.1设备，配置宿主机路由，配置ARP和FDB表里的信息等等
  - 然后，实现网络方案对应的CNI插件。这一部分主要需要做的，就是配置Infra容器里面的网络栈，并把它连接到CNI网桥上

- 接下来，就需要在宿主机上安装flanneld(网络方案本身)，而在这个过程中，flanneld启动后会在每台宿主机上生成它对应的CNI配置文件（它其实是一个ConfigMap），从而告诉K8s，这个集群要使用Flannel作为容器网络方案。

  - CNI配置文件内容如下所示

    ```shell
    
    $ cat /etc/cni/net.d/10-flannel.conflist 
    {
      "name": "cbr0",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
    ```

- 在k8s中，处理容器相关的逻辑并不会在kubelet主干代码里执行，而是会在具体的CRI（Container Runtime Interface，容器运行时接口）实现里完成。对于Docker项目来说，它的CRI实现叫做dockershim。

  - 因此接下来，dockershim会加载上述的CNI配置文件。
  - k8s目前不支持CNI插件混用，如果你在CNI配置目录(/etc/cni/net.d)里放置了多个CNI配置文件的话，dockershim只会加载按字母顺序排列的第一个插件
  - CNI允许你在一个CNI配置文件里，通过plugins字段，定义多个插件进行协作
  - 比如在上面的例子，Flannel项目就制定了flannel和portmap这两个插件，dockershim会把这个CNI配置文件加载起来，并且把列表里的第一个插件，也就是flannel插件，设置为默认插件。在后面的执行过程中，flannel和portmap插件会按照定义顺序被调用，从而依次完成“配置容器网络”和“配置端口映射”这两步操作。

- 接下来，讲解一下CNI插件的工作原理

  - 当kubelet组件需要创建Pod的时候，它第一个创建的一定是Infra容器，所以在第一步，dockershim就会先调用Docker API创建并启动Infra容器，紧接着执行一个叫做SetUpPod的方法。这个方法的作用就是：为CNI插件准备参数，然后调用CNI插件为Infra容器配置网络

  - 这里调用的插件，就是/opt/cni/bin/flannel；而调用它所需要的参数分为两部分：

    - 第一部分，是由dockershim设置的一组CNI环境变量。其中，重要的环境变量：CNI_COMMAND，它的取值有两种ADD和DEL。ADD和DEL操作就是CNI插件唯一需要实现的两个方法。

      - ADD操作：把容器添加到CNI网络里
      - DEL操作：把容器从CNI网络里移除

      对于网桥类型的CNI插件来说，这两个操作意味着把容器以Veth Pair的方式插入到CNI网桥上，或者从网桥上拔掉。

      - 以ADD操作为重点进行讲解：CNI的ADD操作需要的参数包括：容器里网卡的名字eth0(CNI_IFNAME)、Pod的Network Namespace文件的路径(CNI_NETNS)、容器的ID(CNI_CONTAINERID)等。这些参数都属于上述环境变量的内容。其中，Pod(Infra容器)的Network Namespace文件的路径，即：/proc/容器进程的PID/ns/net
      - 除此之外，在CNI环境变量里，还有一个叫做CNI_ARGS的参数，通过这个参数，CRI实现(比如dockershim)就可以以key-value的格式，传递自定义信息给网络插件。这是用户将自定义CNI协议的一个重要方法

    - 第二部分，则是dockershim从CNI配置文件里加载到的，默认插件的配置信息。

      - 配置信息在CNI中被叫做Network Configuration，它的完整定义可以参考https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration dockershim会把Network Configuration以JSON数据的格式，通过标准输入(stdin)的方式传递给Flannel CNI插件

  - 有了这两部分参数，Flannel CNI插件实现ADD操作的过程就非常简单了。

  - 需要注意的是，Flannel的配置文件（/etc/cni/net/d/10-flannel.conflist）里有这么一个字段，叫做delegate：

    ```shell
    ...
         "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
    ```

    Delegate字段的意思是，这个CNI插件并不会自己做事儿，而是会调用Delegate指定的某种CNI内置插件来完成。对于Flannel来说，它调用的Delegate插件，就是前面介绍的CNI bridge插件。

  - 因此，dockershim对Flannel CNI插件的调用，其实就是走了一个过场，Flannel CNI插件唯一需要做的，就是对dockershim传来的Network Configuration进行补充。比如，将Delegate的Type字段设置为bridge，将Delegate的IPAM字段设置为host-local等

    - 经过Flannel CNI插件补充过的，完整的Delegate字段如下所示：

      ```shell
      {
          "hairpinMode":true,
          "ipMasq":false,
          "ipam":{
              "routes":[
                  {
                      "dst":"10.244.0.0/16"
                  }
              ],
              "subnet":"10.244.1.0/24",
              "type":"host-local"
          },
          "isDefaultGateway":true,
          "isGateway":true,
          "mtu":1410,
          "name":"cbr0",
          "type":"bridge"
      }
      ```

    - ipam字段里的信息，比如10.244.1.0/24，读取自Flannel在宿主机上生成的Flannel的配置文件，即：宿主机上的/run/flannel/subnet.env文件

  - 接下来，Flannel CNI插件就会调用CNI bridge插件，也就是执行：/opt/cni/bin/bridge二进制文件

    - 调用bridge插件需要的两部分参数的第一部分，也就是cni环境遍历那个，并没有变化，所以，CNI_COMMAND参数的值还是“ADD”
    - 而第二部分Network Configuration，正是上面补充好的Delegate字段。Flannel CNI插件会把Delegate字段的内容以标准输入(stdin)的方式传递给CNI bridge插件
      - 此外，Flannel CNI插件还会把Delegate字段以json形式保存在/var/lib/cni/flannel目录下，为了给后面删除容器DEL操作时使用
    - 有了这两部分参数，CNI bridge插件就可以代表Flannel，进行"将容器加入到CNI网络里"这一步操作了。而这一部分内容，与容器Network Namespace密切相关。

  - CNI bridge插件会在宿主机上检查CNI网桥是否存在。如果没有的话，就创建它。相当于在宿主机上执行：

    ```shell
    # 在宿主机上
    $ ip link add cni0 type bridge
    $ ip link set cni0 up
    ```

  - 接下来，CNI bridge插件会通过Infra容器的Network Namespace文件，进入到这个Network Namespace里面，创建一对Veth Pair设备

    - 紧接着，把Veth Pair的其中一端，移动到宿主机上，这相当于在容器里执行如下所示的指令：

      ```shell
      #在容器里
      
      # 创建一对Veth Pair设备。其中一个叫作eth0，另一个叫作vethb4963f3
      $ ip link add eth0 type veth peer name vethb4963f3
      
      # 启动eth0设备
      $ ip link set eth0 up 
      
      # 将Veth Pair设备的另一端（也就是vethb4963f3设备）放到宿主机（也就是Host Namespace）里
      $ ip link set vethb4963f3 netns $HOST_NS
      
      # 通过Host Namespace，启动宿主机上的vethb4963f3设备
      $ ip netns exec $HOST_NS ip link set vethb4963f3 up 
      ```

    - 这样，vethb4963f3就出现在了宿主机上，而且这个Veth Pair设备的另一端，就是容器里面的eth0

    也可以在宿主机上创建Veth Pair设备，然后把另一端放到容器的network namespace里；不过CNI插件之所以要反着来，是因为CNI里对Namespace操作函数的设计就是如此，如下所示：

    ```shell
    err := containerNS.Do(func(hostNS ns.NetNS) error {
      ...
      return nil
    })
    ```

    这个设计其实很容易理解。在编程时，容器的Namespace是可以通过Namespace文件拿到的，而Host namespace，则是一个隐含在上下文的参数。所以，先通过容器namespace进入容器里面，然后反向操作host namespace，对于变成来说更加方便

  - 接下来CNI bridge插件就可以把vethb4963f3设备连接在CNI网桥上。这相当于在宿主机上执行：

    ```shell
    # 在宿主机上
    $ ip link set vethb4963f3 master cni0
    ```

  - 在将vethb4963f3设备连接到CNI网桥之后，CNI bridge插件还会为它设置Hairpin Mode(发夹模式)。这是因为，在默认情况下，网桥设备是不允许一个数据包从一个端口进来后，再从这个端口发出去的。但是它可以取消这个限制

    - 这个特性主要用在容器需要通过端口映射的方式，"自己访问自己"的场景下。假设我们执行docker run -p 8080:80，就是在宿主机上设置了一条DNAT的转发规则，将宿主机的IP+8080端口，换成容器ip地址+80端口。也就是说这个请求最终会经过docker0网桥进入容器里。
    - 但如果你在容器里访问宿主机8080端口，这个容器里发出的IP包会经过vethb4963f3端口进入docker0网桥来到宿主机上，并且通过vethb4963f3端口进入到容器里，这里就需要开启Hairpin Mode了
    - 所以Flannel插件要在CNI配置文件里声明hairpinMode=true。这样将来这个集群里的Pod才可以通过它自己的service访问到自己

  - 接下来，CNI bridge插件会调用CNI ipam插件，从ipam.subnet字段规定的网段里为容器分配一个可用的IP地址。然后CNI bridge插件就会把这个IP地址添加到容器的eth0网卡上，同时为容器设置默认路由，这相当于在容器里执行：

    ```shell
    # 在容器里
    $ ip addr add 10.244.0.2/24 dev eth0
    $ ip route add default via 10.244.0.1 dev eth0
    ```

  - 最后，CNI插件会为CNI网桥添加IP地址，这相当于在宿主机上执行：

    ```shell
    # 在宿主机上
    $ ip addr add 10.244.0.1/24 dev cni0
    ```

  - 执行完上述操作后，CNI插件会把容器的IP地址等信息返回给dockershim，然后被kubelet添加到Pod的Status字段。

  - 至此，CNI插件的ADD方法就宣告结束了

  - 接下来的流程就跟我们上一篇文章中容器跨主机通信过程完全一致了

- 需要注意的是，对于非网桥类型的CNI插件，上述“将容器添加到CNI网络”的操作流程，以及网络方案本身的工作原理就都不一样了

## 35 K8s的三层网络方案

- 个人理解：讲了Flannel的host-gw模式和Calico项目
- 个人理解：Flannel的host-gw模式，通过配置下一跳路由，将mac地址改为目的宿主机地址，直接通过宿主机二层网络即可
- 个人理解：Calico项目，原理与host-gw几乎一致，不同的是，采用了BGP协议来维护路由信息，同时可以使用IPIP模式打破host-gw模式只能在宿主机二层连通的局限性，但同时IPIP模式中多了三层封包解包，因此性能差一些，和Flannel VXLAN性能差不多



纯三层网络方案，典型例子：Flannel的host-gw模式和Calico项目

### Flannel的host-gw模式

![image-20200308031353497](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200308031353497.png)

- 工作原理：将每个Flannel子网的“下一跳”设置成该子网对应的宿主机的IP地址，即目的宿主机会充当这条容器通信路径里的“网关”，这也正是host-gw名字的含义
- 核心：其实就是IP包在封装成帧发送出去的时候，会使用路由表里的“下一跳”来设置目的MAC地址，这样就会经过宿主机的二层网络到达目的宿主机
- 性能：host-gw的性能损耗大约在10%左右，而其他所有基于VXLAN“隧道”机制的网络方案，性能损失都在20%～30%左右
- 路由信息维护：路由信息的维护由flanneld进程进行维护，Flannel子网和主机的信息都是保存在Etcd中，flanneld只需要WATCH这些数据的变化，然后实时更新路由表即可
- 缺点：要求集群宿主机之间二层连通

### Calico项目

像Flannel host-gw这样的三层网络方案，龙头老大应该算是Calico项目

- 不同点：网络解决方案与Flannel host-gw几乎完全一样，不同的是Flannel通过Etcd和宿主机上的flanneld来维护路由信息，而Calico项目使用了BGP来自动地在整个集群中分发路由信息
- BGP Border Gateway Protocol，即边界网关协议。Linux内核原生支持，专门用在大规模数据中心维护不同的“自治系统”之间路由信息的，无中心的路由协议。其实BGP就是在大规模网络中实现节点路由信息共享的一种协议。
- BGP的能力，正好可以取代Flannel维护主机上路由表的功能，而且BGP这种原生就是为大规模网络环境而实现的协议，其可靠性和可扩展性，远非Flannel自己的方案可比

Calico项目架构由三部分组成：

- Calico的CNI插件：与K8s对接的部分，在上一讲有讲到
- Felix：一个DaemonSet，负责在宿主机插入路由规则（即，写入Linux内核的FIB转发信息库），以及维护Calico所需的网络设备等工作
- BIRD：BGP的客户端，专门负责在集群里分发路由规则信息

#### 工作原理

除了对路由信息的维护方式外，Calico项目与Flannel的host-gw模式的另一个不同之处在于，他不会在宿主机上创建任何网桥设备。以下是Calico的工作原理示意图：

![image-20200308033216784](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200308033216784.png)

- Calico的CNI插件会为每个容器设置一个Veth Pari设备，然后其中一端放在宿主机上，名字前缀是cali

- 由于Calico没使用CNI的网桥模式，Calico的CNI插件还需在宿主机上为每个容器的Veth Pair设备配置一条路由规则，用于接收传入的IP包，比如，宿主机Node2上的Container4对应的路由规则为：

  ```shell
  # 发往10.233.2.3的IP包，应该进入cali5863f3设备
  10.233.2.3 dev cali5863f3 scope link
  ```

  因此Calico在宿主机设置的路由规则肯定比Flannel多

- 在三大组件里说过，下一跳的路由规则由Felix进程负责维护。路由规则信息，通过RIRD组件，使用BGP协议传输而来。BGP协议传输信息可以简单理解为以下格式：

  ```wiki
  [BGP消息]
  我是宿主机192.168.1.3
  10.233.2.0/24网段的容器都在我这里
  这些容器的下一跳地址是我
  ```

  也就是说，Calico实际上将所有节点都当作边界路由器来处理，节点之间通过BGP协议交换路由规则，这些节点，我们称为BGP Peer

- Calico维护的网络默认配置下，是“Node-to-Node Mesh”模式，每台宿主机的BGP Client都需要跟其他所有节点的BGP Client进行通信来交换路由信息，随着节点的增加，链接数量以N^2的规模快速增长。一般推荐使用少于100个节点的集群，而在更大规模的集群中，需要使用Route Reflector模式

- Route Reflector模式，Calico会指定一个或者几个专门的节点，来负责跟所有节点建立BGP连接从而学习全聚德路由规则，其他节点只需要和这几个专门的节点交换路由信息即可，因此连接数量控制在了N的数量级

#### IPIP模式

如果宿主机之间不是二层连通，可以开启Calico的IPIP模式

![image-20200308040553968](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200308040553968.png)

- Felix进程会在Node1上添加路由规则

  ```shell
  10.233.2.0/24 via 192.168.2.2 tunl0
  ```

  tunl0设备，与Flannel UDP模式使用tun设备功能完全不同，tunl0设备是一个IP隧道设备

- IP包进入IP隧道设备之后，会被Linux内核的IPIP驱动接管，IPIP驱动会将这个IP包直接封装在一个宿主机网络的IP包中

  ![image-20200308040938817](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200308040938817.png)

  新的IP包的目的地址，正是原IP包的下一跳地址，即Node2的IP地址：192.168.2.2

  - 原先从容器到Node2的IP包就被伪装成一个从Node1到Node2的IP包
  - 由于宿主机之间使用路由器配置了三层转发，因此，这个IP包离开Node1之后，就可以经过路由器，最终跳到Node2上

- 到达Node2后，Node2的网络内核栈会使用IPIP驱动进行解包，从而拿到原始的IP包。然后，原始IP包就会经过路由规则和Veth Pair设备到达目的容器内部

以上就是Calico的工作原理

- IPIP模式的性能：因为有额外的封包解包性能有所下降，实际测试中，Calico IPIP模式与Flannel VXLAN模式的性能大致相当
- 因此建议将所有宿主机节点放在一个子网里，避免使用IPIP模式



思考宿主机不是二层连通的其他解决方案：

如果Calico项目可以让宿主机之间的路由设备(也就是网关),也通过BGP协议学习到Calico网络里的路由规则，那么从容器发出的IP包，就可以通过这些设备路由到目的宿主机了

比如，在上面IPIP工作原理示意图中的Node1中，添加路由：

```shell
10.233.2.0/24 via 192.168.1.1 eth0
```

然后，在Router1上（192.168.1.1），添加如下所示的路由规则：

```shell
10.233.2.0/24 via 192.168.2.1 eth0
```

那么，Container1发出的IP包，就可以通过两次“下一跳”，到达Route2(192.168.2.1)了，同理，继续在Router2上添加下一跳路由，最终把IP包转发到Node2上



遗憾的是：在公有云环境中，宿主机之间的网关，肯定不会允许用户进行干预和设置

当然，在大多数公有云环境中，宿主机（公有云提供的虚拟机）本身往往就是二层连通

不过，如果是私有部署的环境，宿主机属于不同子网，想办法将宿主机网关也加入到BGP Mesh里从而避免使用IPIP成了一个迫切需求



在Calico项目中，提供了两种将宿主机网关设置成BGP Peer的解决方案：

- 1，所有宿主机都跟宿主机网关建立BGP Peer关系

  这种方案下，要求宿主机网关支持Dynamic Neighbors的BGP配置方式，因为常规的BGP配置里，运维人员必须明确给出所有BGP Peer的IP地址。考虑到k8s可能宿主机很多，会动态添加删除节点，手动管理路由器的BGP配置很麻烦。而Dynamic Neighbors则允许你给路由器配置一个网段，然后路由器就会自动跟盖网段里所有主机建立BGP Peer关系

- 2，使用一个或多个独立组件负责搜集整个集群的所有路由信息，然后通过BGP协议同步给网关。可以使用Route Reflector节点兼任，负责跟宿主机网关机型沟通

  由于这种情况，BGP Peer个数有限且固定，可以直接配置成路由器的BGP Peer，而无需Dynamic Neighbors的支持

  这些独立组件只需要WATCH Etcd里的宿主机和对应网段的变化信息，然后把信息通过BGP协议分发给网关即可

## 36 为什么说K8s只有soft multi-tenancy

- k8s中网络隔离采用network policy这个资源来控制，原理就是通过将配置的规则映射到宿主机的iptables规则

前面讲的都是容器之间的连通，在k8s中是如何考虑网络的隔离的呢？

### NetWork Policy

在k8s中，网络隔离能力的定义是依靠一种专门的API对象来描述的，即，NetworkPolicy

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

- Pod默认是可以接收来自任何发送方的请求的，或者说向任何接收方发送请求，要做出限制，就需要通过NetworkPolicy对象来指定
- PodSelcetor字段：定义Networkpolicy的限制范围，如果该字段为空，就会作用于当前namespace的所有Pod。一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。NetworkPolicy其实是一个白名单

- 如果要让上面定义的NetworkPolicy在Kubernetes集群中真正产生作用，CNI网络插件就必须支持Kubernetes的NetworkPolicy
- 具体实现上，凡是支持NetworkPolicy的CNI插件，都维护着一个NetworkPolicy Controller，通过控制循环的方式对NetworkPolicy对象的增删改查做出响应，然后在宿主机上完成iptables规则的配置工作
- 在Kubernetes生态里，目前已经实现了networkPolicy的网络插件包括Calico，Weave和kube-router等多个新手该墓，但不包括flannel项目
- 因此如果可以使用Flannel+Calico来做网络方案



### 原理

以三层网络插件为例（比如calico和kube-router）

```yaml
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
   - from:
     - namespaceSelector:
         matchLabels:
           project: myproject
     - podSelector:
         matchLabels:
           role: frontend
     ports:
       - protocol: tcp
         port: 6379
```

- CNI插件会使用这个NetworkPolicy的定义，在宿主机上生成iptbales规则

  ```go
  
  for dstIP := range 所有被networkpolicy.spec.podSelector选中的Pod的IP地址
    for srcIP := range 所有被ingress.from.podSelector选中的Pod的IP地址
      for port, protocol := range ingress.ports {
        iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT 
      }
    }
  } 
  ```

  - CNI网络插件对Pod进行隔离，其实是靠在宿主机上生成NetworkPolicy对应的iptables规则来实现的

- 设置好上述隔离规则后，网络插件还需要将所有对被隔离Pod的访问请求，都转发到上述KUBE-NWPLCY-CHAIN规则上去进行匹配。并且如何匹配不通过，这个请求应该被拒绝。因此需要设置两组规则：

  - 第一组：将请求转发到KUBE-POD-SPECIFIC-FW-CHAIN（第一条规则是本机的流量，因此判断网桥设备，走的是iptables的二层，见图；第二条规则是正常的流量）

    ![img](%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/32b93250b62059498481f72cc9c1a6be.png)
  
  ```go
  
  for pod := range 该Node上的所有Pod {
      if pod是networkpolicy.spec.podSelector选中的 {
          iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN
          iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN
          ...
      }
}
  ```

  - 第二组，将请求转发到KUBE-NWPLCY-CHAIN进行匹配，没有匹配到的拒绝请求（第一条规则是匹配，匹配上的话就会接受，第二条规则是如果没匹配上之后的拒绝）
  
  ```shell
  
  iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN
iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable
  ```
  
  

## 37 找到容器不容易：Service，DNS与服务发现

- 个人理解：Service通过iptables或ipvs模式，通过VIP访问，进行dnat，将目的地址修改为Pod的地址。
- 个人理解：对于ClusterIP模式的service，访问svc-name.ns-name.svc.cluster.local，通过DNS解析为VIP，从而进行服务；对于ClusterIP=None的headless service来说，访问svc-name.ns-name.svc.cluster.local，会解析到Pod的地址集合

### service

- 需求

  - Pod的IP不是固定的
  - 一组Pod实例之间总会有负载均衡的需求

- service例子

  ```yaml
  apiVersion: v1
  kind: service
  metadata:
  	name: hostnames
  spec:
  	selector:
  		app: hostnames
  	ports:
  	- name: default
  		protocol: TCP
  		port: 80
  		targetPort: 9376
  ```

  - Endpoints

    - 被selector选中的Pod

    ```shell
    $ kubectl get endpoints hostnames
    NAME        ENDPOINTS
    hostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
    ```

- 通过Service的VIP地址10.0.1.175，就可以访问service代理的Pod了

  ```shell
  $ kubectl get svc hostnames
  NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
  hostnames   ClusterIP   10.0.1.175   <none>        80/TCP    5s
  
  $ curl 10.0.1.175:80 # 该应用是返回主机名
  hostnames-0uton
  
  $ curl 10.0.1.175:80
  hostnames-yp2kp
  
  $ curl 10.0.1.175:80
  hostnames-bvc05
  ```

  - VIP地址是k8s自动为service分配的，三次访问，依次返回了三个Pod的hostname，说明service使用了Round Robin方式负载均衡，这种方式，成为ClusterIP模式的Service

### service - clusterIP实现

- service 是由 kube-proxy + iptables共同实现的

- 一旦service被创建，就会在宿主机创建下面这条iptables

  ```shell
  -A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3
  ```

  - 凡是目的地址是10.0.1.175，目的端口是80的IP包，就跳转到一条名叫KUBE-SVC-NWV5X2332I4OT4T3的iptables链

  ```shell
  
  -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
  -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
  -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR
  ```

  ```shell
  
  -A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
  -A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.3.6:9376
  
  -A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
  -A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.1.7:9376
  
  -A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
  -A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.2.3:9376
  ```

  - 这些Endpoints对应的iptables规则，正是kube-proxy通过监听Pod的变化事件在宿主机生成并维护的

- IPVS模式

  与iptables模式类似，在创建了service后，kube-proxy首先会在宿主机上创建一个虚拟网卡 kube-ipvs0，并为它分配service VIP的ip地址

  ```shell
  # ip addr
    ...
    73：kube-ipvs0：<BROADCAST,NOARP>  mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
    inet 10.0.1.175/32  scope global kube-ipvs0
    valid_lft forever  preferred_lft forever
  ```

  接下来，kube-proxy通过linux的IPVS模式，为这个IP地址设置三个IPVS虚拟主机，并设置这三个虚拟主机之间使用轮询模式来作为负载均衡策略，可以通过ipvsadm查看这个设置：

  ```shell
  
  # ipvsadm -ln
   IP Virtual Server version 1.2.1 (size=4096)
    Prot LocalAddress:Port Scheduler Flags
      ->  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     
    TCP  10.102.128.4:80 rr
      ->  10.244.3.6:9376    Masq    1       0          0         
      ->  10.244.1.7:9376    Masq    1       0          0
      ->  10.244.2.3:9376    Masq    1       0          0
  ```

  这时候，任何发往10.102.128.4:80的请求就会被IPVS转发到某一个后端Pod上。

  - 相比iptables，IPVS在内核中其实也是基于Netfilter的NAT模式，所以在转发这一层，理论上并没有显著的性能提升
  - 但IPVS并不需要在宿主机为每个Pod设置iptables规则，而是把这些规则的处理放到了内核态，从而大大降低了维护代价

  ipvs模块只负责负载均衡和代理功能，而一个完整的service流程正常工作所需要的包过滤，SNAT等操作，还是要靠iptables实现，只不过，这些辅助性的iptables规则数量有限，不会随着Pod数量增加而增加

### DNS解析

在K8s中，Service和Pod都会被分配对应的DNS A记录

- 对于ClusterIP模式的service来说,A记录的格式为....svc.cluster.local，访问时，解析到的就是service的VIP地址
- 对于Cluster=None的headless service来说，它的A记录格式也是....svc.cluster.local，当访问A记录时，返回的是被代理的Pod的IP地址集合，当客户端没办法解析这个集合的时候，可能智慧拿到第一个Pod的IP地址
- 对于ClusterIP模式的Service来说，它代理的Pod被自动分配的A记录的格式是：....pod.cluster.local，这条记录对应Pod的IP
- 对于headless service来说，代理的Pod被自动分配的A记录的格式为：....svc.cluster.local，这条记录也指向Pod的IP地址

## 38 从外界连通Service与Service调试“三板斧”

- 个人理解-NodePort：通过访问任意一台宿主机的IP:NodePort，可以将请求通过Iptables规则负载均衡到不同的Pod后端。
  - 这种方式需要做SNAT，因为客户端发给Node1的请求有可能发到了Node2上的Pod，如果不做SNAT，客户端接收到的回复的源地址与自己发送包的目的地址不一致，会出错；
  - 也可以不做SNAT，将service的spec.externalTrafficPolicy字段设置为local，这就保证所有Pod通过Service收到请求之后一定能看到真正的外部client的源地址，这是因为访问的宿主机与最终回应请求的Pod必须是一台宿主机，如果该宿主机没有相应的Pod，则会DROP掉请求
- 个人理解-Loadblancer模式：基于NodePort模式，使用CloudProvider在公有云上创建一个负载均衡器，后台是不同宿主机:NodePort
- 个人理解-ExternalName模式：通过设置kube-dns的CNAME记录，访问服务域名my-service.ns-name.svc.cluster.local与访问externalName属性规定的域名达到的效果一致；
  - 除此之外，通过设置externalIPs，来通过ip访问到后台的Pod服务，但externalIPs必须能访问到k8s中的一个节点
- 个人理解-service相关的问题如何解决（暂时还没看）

### 外部访问Service-NodePort

- 需求来源

  - 对于ClusterIP模式的Service来说，访问入口其实就是每台宿主机上由kube-proxy生成的iptables规则，以及kube-dns生成的DNS记录，而一旦离开这个集群，这些信息对用户就没用了
  - 如何从外部（k8s集群之外）访问到k8s里创建的Service呢？

- NodePort

  ```yaml
  
  apiVersion: v1
  kind: Service
  metadata:
    name: my-nginx
    labels:
      run: my-nginx
  spec:
    type: NodePort
    ports:
    - nodePort: 8080
      targetPort: 80
      protocol: TCP
      name: http
    - nodePort: 443
      protocol: TCP
      name: https
    selector:
      run: my-nginx
  ```

  - 访问这个Service，只需要访问

    ```text
    <任何一台宿主机的IP地址>:8080
    ```

    就可以访问Pod的80端口了

  - 原理：将访问8080端口的请求发送到KUBE-SVC-NWV5X2332I4OT4T3链，该链其实是一组随机模式的iptables规则，接下来就和ClusterIP模式一样了

    ```shell
    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/my-nginx: nodePort" -m tcp --dport 8080 -j KUBE-SVC-NWV5X2332I4OT4T3
    ```

  - 注意：在NodePort方式下，K8s会在IP包离开宿主机发往目的Pod时，对这个IP包做一次SNAT操作，如下所示：

    ```shell
    -A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x4000/0x4000 -j MASQUERADE
    ```

    可以看到，在它即将离开这台主机的IP包，进行了一次SNAT操作，将这个IP包的源地址替换成了这台宿主机的CNI网桥地址，或者宿主机本身的IP地址(如果CNI网桥不存在的话)

    - 由于SNAT操作只需对Service转发来的IP包进行，因此需要查看IP包是否有一个“0x4000”的标志，这正是IP包在执行DNAT操作之前被打上去的

    - 为什么要做ANAT

      <img src="%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.assets/image-20200310222850592.png" alt="image-20200310222850592" style="zoom:50%;" />

      当Client通过node2的地址访问到一个service的时候，node2上的负载均衡规则，可能会把IP包转发给一个在node1上的Pod；而当node1上这个Pod处理完请求后，就会按照这个IP包的源地址发出回复，即直接发给客户端，这时候客户端收到后的IP包源地址与自己发送时候的目的地址不一致，client可能会报错

  - 但SNAT的方式会导致Pod只知道该IP包来自node2，对于Pod需要明确知道请求来源的场景来说，这是不可以的

    - 可以将service的spec.externalTrafficPolicy字段设置为local，这就保证所有Pod通过Service收到请求之后一定能看到真正的外部client的源地址
    - 实现原理：一台宿主机上的Iptables规则，会设置只将IP包转发给运行在这台宿主机上的Pod，所以这时候Pod可以直接使用源地址回复包，不用使用SNAT了
    - 缺点：如果访问的宿主机上没有一个被代理的Pod存在，请求会直接被DROP掉

### 外部访问Service-LoadBalancer

适用于公有云上的Kubernetes服务

```yaml
kind: Service
apiVersion: v1
metadata:
  name: example-service
spec:
  ports:
  - port: 8765
    targetPort: 9376
  selector:
    app: example
  type: LoadBalancer
```

在NodePort的基础上，在LoadBalancer类型的Service被提交后，Kubernetes就会调用CloudProvider在公有云上为你创建一个负载均衡服务，并将请求转发到< NodeIP > : NodePort



### 外部访问Service-ExternalName

```yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  type: ExternalName
  externalName: my.database.example.com
```

- 没有指定selector
- 当通过service的DNS名字访问它的时候，比如访问my-service.default.svc.cluster.local，k8s为你返回的就是my.database.example.com
- 原理
  - 在kube-dns里添加了一条CNAME记录，访问my-service.default.svc.cluster.local和访问my.database.example.com这个域名是一个效果



- service还允许你为service分配公有IP地址，比如下面这个例子：

  ```yaml
  kind: Service
  apiVersion: v1
  metadata:
    name: my-service
  spec:
    selector:
      app: MyApp
    ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    externalIPs:
    - 80.11.12.10
  ```

  - 通过访问80.11.12.10:80访问到被代理的Pod了
  - externalIPs必须是至少能够路由到一个Kubernetes的节点



### service相关的问题（没看）



## 39 谈谈Service与Ingress

- 个人理解-Ingress：通过域名，代理后台k8s服务，相当于服务的“服务”，ingress支持七层，service支持4层
- 个人理解-Ingress-nginx-controller：通过ingress nginx controller维护一个nginx服务，自动根据Ingress对象的rules内容更新nginx配置文件

### Ingress

- 需求来源

  - LoadBalancer类型的Service，会为你在Cloud Provider里创建一个该Service对应的负载均衡服务。每个service都要一个负载均衡服务，做法既浪费成本又高
  - 希望看到k8s为我内置一个全局的负载均衡器，然后，通过我访问的URL，把请求转发给不同的后端Service
  - 这种全局的，为了代理不同后端Service而设置的负载均衡服务，就是k8s里的Ingress服务，所谓Ingress，就是Service的service

- Ingress对象

  ```yaml
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: cafe-ingress
  spec:
    tls:
    - hosts:
      - cafe.example.com
      secretName: cafe-secret
    rules:
    - host: cafe.example.com
      http:
        paths:
        - path: /tea
          backend:
            serviceName: tea-svc
            servicePort: 80
        - path: /coffee
          backend:
            serviceName: coffee-svc
            servicePort: 80
  ```

  - 所谓Ingress对象，其实就是k8s项目对“反向代理” 的一种抽象

### Nginx Ingress Controller

Ingress机制的使用过程

- 部署Nginx Ingress Controller

  ```shell
  $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
  ```

  - 在mandatory.yaml这个文件里，正是Nginx官方维护的Ingress Controller的定义
  - yaml文件解释：里定义了一个使用nginx-ingress-controller镜像的Pod，是一个用来监听Ingress对象以及它所代理的后端Service变化的控制器
  - 原理：当一个新的Ingress对象由用户创建后，nginx-ingress-controller会根据Ingress对象里定义的内容生成一份对应的Nginx配置文件，并使用这个配置文件启动一个nginx服务
    - 一旦Ingress对象被更新，nginx-ingress-controller就会更新这个配置文件，nginx服务是不需要重新加载的，nginx-ingress-controller通过nginx lua方案实现了nginx upstream的动态配置
  - 定制nginx配置：nginx-ingress-controller还允许通过ConfigMap对象来对nginx配置文件进行定制，ConfigMap的名字需要以参数方式传递给nginx-ingress-controller，在configMap里添加的字段将会合并到最后生成的Nginx配置文件里

- 本质：Nginx ingress Controller提供的服务，是可以根据Ingress对象和被代理后端service的变化，自动进行更新的nginx负载均衡器

- 暴露nginx服务：为了让用户可以访问nginx，需要创建一个service来把nginx ingress controller管理的nginx服务暴露出去（Bare-metal环境可以使用NodePort模式，公有云环境可以使用loadbalancer）

- 部署完Ingress controller和所需要的service部署完成后，就可以使用了，接下来部署后台服务即可

- https访问：由于想要用户通过https访问，需要创建Ingress所需要的SSL证书(pls.crt)和密钥(pls.key)，这些信息通过secret配置到ingress中即可

- 匹配失败的默认服务：如果请求没有匹配到任何一条IngressRule，就会返回一个Nginx的404页面，Ingress Controller也可以通过Pod启动命令里的-default-backend-service参数设置一条默认规则，比如-default-backend-service=nginx-default-backend，这样就可以将匹配失败的请求由nginx-default-backend这个服务来处理了



